{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Brand-Safety Toxicity Analysis: Wikipedia Comments as Proxy for Ad Creative Risk\"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import io\n",
        "\n",
        "print(\"ðŸ“¤ Step 1: Upload your test.csv file\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Let's upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… File '{filename}' uploaded successfully!\")\n",
        "print(f\"ðŸ“ File size: {len(uploaded[filename]) / 1024:.2f} KB\")\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "print(f\"ðŸ“Š Dataset loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "print(f\"ðŸ“‹ Columns: {list(df.columns)}\")\n",
        "\n",
        "# Here's basic info about the dataset\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“ˆ DATASET OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(df.info())\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# NLP Libraries\n",
        "print(\"\\nâ³ Installing required NLP libraries...\")\n",
        "!pip install textblob vaderSentiment detoxify\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from detoxify import Detoxify\n",
        "\n",
        "# Initializing sentiment analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"âœ… All libraries installed and imported successfully!\")\n",
        "\n",
        "# Sampling the dataset (using 1000 rows for faster analysis)\n",
        "def sample_data(df, n_samples=1000, random_state=42):\n",
        "    \"\"\"Sample the dataset for analysis\"\"\"\n",
        "    if len(df) > n_samples:\n",
        "        sampled_df = df.sample(n=n_samples, random_state=random_state)\n",
        "        print(f\"ðŸ“Š Sampled {n_samples} rows from dataset for analysis\")\n",
        "    else:\n",
        "        sampled_df = df.copy()\n",
        "        print(f\"ðŸ“Š Using full dataset ({len(df)} rows) for analysis\")\n",
        "    return sampled_df\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text cleaning for analysis\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "    # Let's remove Wikipedia markup and special characters\n",
        "    text = re.sub(r'==+.*?==+', '', text)  # Remove section headers\n",
        "    text = re.sub(r'\\[\\[.*?\\|?(.*?)\\]\\]', r'\\1', text)  # Simplify links\n",
        "    text = re.sub(r'{{.*?}}', '', text)  # Remove templates\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:]', '', text)  # Keep basic punctuation\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Sentiment and toxicity analysis\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"Analyze sentiment using VADER\"\"\"\n",
        "    scores = vader_analyzer.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "def analyze_sentiment_textblob(text):\n",
        "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "def analyze_toxicity_detoxify(text):\n",
        "    \"\"\"Analyze toxicity using Detoxify\"\"\"\n",
        "    try:\n",
        "        results = Detoxify('original').predict(text)\n",
        "        return results\n",
        "    except:\n",
        "        return {'toxicity': 0, 'severe_toxicity': 0, 'obscene': 0,\n",
        "                'threat': 0, 'insult': 0, 'identity_attack': 0}\n",
        "\n",
        "# Risk classification\n",
        "def classify_risk_level(row, toxicity_threshold=0.7, neg_sentiment_threshold=-0.5):\n",
        "    \"\"\"Classify comments into brand-risk categories\"\"\"\n",
        "    toxicity_risk = row['toxicity'] > toxicity_threshold\n",
        "    sentiment_risk = row['vader_compound'] < neg_sentiment_threshold\n",
        "\n",
        "    if toxicity_risk and sentiment_risk:\n",
        "        return 'High Risk'\n",
        "    elif toxicity_risk or sentiment_risk:\n",
        "        return 'Medium Risk'\n",
        "    else:\n",
        "        return 'Low Risk'\n",
        "\n",
        "def main_analysis(df):\n",
        "    \"\"\"Main analysis function\"\"\"\n",
        "    # Sample data for analysis\n",
        "    analysis_df = sample_data(df)\n",
        "\n",
        "    # Preprocess text\n",
        "    print(\"ðŸ”„ Preprocessing text...\")\n",
        "    analysis_df['cleaned_text'] = analysis_df['comment_text'].apply(preprocess_text)\n",
        "    analysis_df['text_length'] = analysis_df['cleaned_text'].str.len()\n",
        "\n",
        "    # Filter out very short comments (likely uninformative)\n",
        "    initial_count = len(analysis_df)\n",
        "    analysis_df = analysis_df[analysis_df['text_length'] > 20]\n",
        "    print(f\"ðŸ“ Filtered out {initial_count - len(analysis_df)} short texts\")\n",
        "    print(f\"ðŸ“Š Remaining samples: {len(analysis_df)}\")\n",
        "\n",
        "    print(\"ðŸ” Analyzing sentiment and toxicity...\")\n",
        "\n",
        "    # VADER Sentiment Analysis\n",
        "    print(\"ðŸ“Š Running VADER sentiment analysis...\")\n",
        "    vader_results = analysis_df['cleaned_text'].apply(analyze_sentiment_vader)\n",
        "    analysis_df[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']] = (\n",
        "        pd.DataFrame(vader_results.tolist(), index=analysis_df.index)\n",
        "    )\n",
        "\n",
        "    # TextBlob Sentiment Analysis\n",
        "    print(\"ðŸ“Š Running TextBlob sentiment analysis...\")\n",
        "    textblob_results = analysis_df['cleaned_text'].apply(analyze_sentiment_textblob)\n",
        "    analysis_df[['textblob_polarity', 'textblob_subjectivity']] = (\n",
        "        pd.DataFrame(textblob_results.tolist(), index=analysis_df.index)\n",
        "    )\n",
        "\n",
        "    # Toxicity Analysis (this will take a few minutes)\n",
        "    print(\"âš ï¸ Running toxicity analysis (this may take 5-15 minutes)...\")\n",
        "    toxicity_results = []\n",
        "    for i, text in enumerate(analysis_df['cleaned_text']):\n",
        "        if i % 50 == 0:\n",
        "            print(f\"ðŸ”¬ Processed {i}/{len(analysis_df)} texts...\")\n",
        "        toxicity_results.append(analyze_toxicity_detoxify(text))\n",
        "\n",
        "    toxicity_df = pd.DataFrame(toxicity_results, index=analysis_df.index)\n",
        "    analysis_df = pd.concat([analysis_df, toxicity_df], axis=1)\n",
        "\n",
        "    # Risk Classification\n",
        "    print(\"ðŸŽ¯ Classifying risk levels...\")\n",
        "    analysis_df['risk_level'] = analysis_df.apply(classify_risk_level, axis=1)\n",
        "\n",
        "    print(\"âœ… Analysis complete!\")\n",
        "    return analysis_df\n",
        "\n",
        "# Run the analysis\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸš€ STARTING BRAND-SAFETY ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "analysis_df = main_analysis(df)\n",
        "\n",
        "# Display basic results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“ˆ ANALYSIS RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Total comments analyzed: {len(analysis_df)}\")\n",
        "risk_distribution = analysis_df['risk_level'].value_counts()\n",
        "print(f\"\\nRisk Level Distribution:\")\n",
        "for level, count in risk_distribution.items():\n",
        "    percentage = (count / len(analysis_df)) * 100\n",
        "    print(f\"  {level}: {count} comments ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Average Scores:\")\n",
        "print(f\"  Toxicity: {analysis_df['toxicity'].mean():.3f}\")\n",
        "print(f\"  Sentiment (VADER): {analysis_df['vader_compound'].mean():.3f}\")\n",
        "print(f\"  Text Length: {analysis_df['text_length'].mean():.1f} characters\")\n",
        "\n",
        "# Visualization\n",
        "def create_visualizations(analysis_df):\n",
        "    \"\"\"Create comprehensive visualizations of the results\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Brand-Safety Risk Analysis: Wikipedia Comments as Ad Creative Proxy',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Risk Level Distribution\n",
        "    risk_counts = analysis_df['risk_level'].value_counts()\n",
        "    colors = ['#2ecc71', '#f39c12', '#e74c3c']  # Green, Orange, Red\n",
        "    axes[0,0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', colors=colors)\n",
        "    axes[0,0].set_title('Risk Level Distribution', fontweight='bold')\n",
        "\n",
        "    # 2. Toxicity Score Distribution\n",
        "    axes[0,1].hist(analysis_df['toxicity'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
        "    axes[0,1].axvline(x=0.7, color='darkred', linestyle='--', label='High Risk Threshold (0.7)')\n",
        "    axes[0,1].set_xlabel('Toxicity Score')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].set_title('Toxicity Score Distribution', fontweight='bold')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Sentiment Distribution\n",
        "    axes[0,2].hist(analysis_df['vader_compound'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
        "    axes[0,2].axvline(x=-0.5, color='darkblue', linestyle='--', label='Negative Threshold (-0.5)')\n",
        "    axes[0,2].set_xlabel('VADER Compound Sentiment')\n",
        "    axes[0,2].set_ylabel('Frequency')\n",
        "    axes[0,2].set_title('Sentiment Distribution', fontweight='bold')\n",
        "    axes[0,2].legend()\n",
        "    axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Toxicity vs Sentiment Scatter\n",
        "    colors_map = {'Low Risk': '#2ecc71', 'Medium Risk': '#f39c12', 'High Risk': '#e74c3c'}\n",
        "    scatter = axes[1,0].scatter(analysis_df['vader_compound'], analysis_df['toxicity'],\n",
        "                               c=analysis_df['risk_level'].map(colors_map),\n",
        "                               alpha=0.6, s=50)\n",
        "    axes[1,0].axhline(y=0.7, color='r', linestyle='--', alpha=0.5)\n",
        "    axes[1,0].axvline(x=-0.5, color='b', linestyle='--', alpha=0.5)\n",
        "    axes[1,0].set_xlabel('VADER Sentiment')\n",
        "    axes[1,0].set_ylabel('Toxicity Score')\n",
        "    axes[1,0].set_title('Toxicity vs Sentiment Correlation', fontweight='bold')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Create legend for scatter plot\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_map['High Risk'],\n",
        "                             markersize=8, label='High Risk'),\n",
        "                      Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_map['Medium Risk'],\n",
        "                             markersize=8, label='Medium Risk'),\n",
        "                      Line2D([0], [0], marker='o', color='w', markerfacecolor=colors_map['Low Risk'],\n",
        "                             markersize=8, label='Low Risk')]\n",
        "    axes[1,0].legend(handles=legend_elements)\n",
        "\n",
        "    # 5. Risk by Toxicity Type\n",
        "    toxicity_cols = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']\n",
        "    risk_means = analysis_df.groupby('risk_level')[toxicity_cols].mean()\n",
        "    risk_means.T.plot(kind='bar', ax=axes[1,1], color=colors)\n",
        "    axes[1,1].set_title('Average Toxicity Scores by Risk Level', fontweight='bold')\n",
        "    axes[1,1].set_ylabel('Average Score')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    axes[1,1].legend(title='Risk Level')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Text Length by Risk Level\n",
        "    box_data = [analysis_df[analysis_df['risk_level'] == level]['text_length']\n",
        "               for level in ['Low Risk', 'Medium Risk', 'High Risk']]\n",
        "    box_plot = axes[1,2].boxplot(box_data, labels=['Low', 'Medium', 'High'], patch_artist=True)\n",
        "\n",
        "    # Color the boxes\n",
        "    for patch, color in zip(box_plot['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "\n",
        "    axes[1,2].set_title('Text Length by Risk Level', fontweight='bold')\n",
        "    axes[1,2].set_ylabel('Text Length (characters)')\n",
        "    axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"\\nðŸ“Š Generating visualizations...\")\n",
        "fig = create_visualizations(analysis_df)\n",
        "\n",
        "# High-risk examples analysis\n",
        "def analyze_high_risk_examples(analysis_df, n_examples=5):\n",
        "    \"\"\"Display and analyze high-risk content examples\"\"\"\n",
        "\n",
        "    high_risk_df = analysis_df[analysis_df['risk_level'] == 'High Risk']\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸš¨ HIGH-RISK CONTENT EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "    if len(high_risk_df) == 0:\n",
        "        print(\"No high-risk content found in this sample.\")\n",
        "        return\n",
        "\n",
        "    for i, (idx, row) in enumerate(high_risk_df.head(n_examples).iterrows()):\n",
        "        print(f\"\\nðŸ”´ Example {i+1}:\")\n",
        "        print(f\"   Toxicity: {row['toxicity']:.3f} | Sentiment: {row['vader_compound']:.3f}\")\n",
        "        print(f\"   Text Preview: {row['cleaned_text'][:150]}...\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Analyze high-risk examples\n",
        "analyze_high_risk_examples(analysis_df)\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ“Š STATISTICAL SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nRisk Level Statistics:\")\n",
        "risk_stats = analysis_df.groupby('risk_level').agg({\n",
        "    'toxicity': ['mean', 'std', 'max'],\n",
        "    'vader_compound': ['mean', 'std', 'min'],\n",
        "    'text_length': ['mean', 'std', 'max'],\n",
        "    'insult': ['mean', 'max'],\n",
        "    'obscene': ['mean', 'max']\n",
        "}).round(3)\n",
        "\n",
        "print(risk_stats)\n",
        "\n",
        "# Correlation analysis\n",
        "print(\"\\nCorrelation Matrix (Toxicity vs Sentiment vs Length):\")\n",
        "correlation_matrix = analysis_df[['toxicity', 'vader_compound', 'text_length']].corr()\n",
        "print(correlation_matrix.round(3))\n",
        "\n",
        "# Business implications\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸ’¼ BUSINESS IMPLICATIONS FOR BRAND-SAFETY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_high_risk = len(analysis_df[analysis_df['risk_level'] == 'High Risk'])\n",
        "total_medium_risk = len(analysis_df[analysis_df['risk_level'] == 'Medium Risk'])\n",
        "total_analyzed = len(analysis_df)\n",
        "\n",
        "high_risk_pct = (total_high_risk / total_analyzed) * 100\n",
        "medium_risk_pct = (total_medium_risk / total_analyzed) * 100\n",
        "total_risk_pct = high_risk_pct + medium_risk_pct\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Key Findings:\")\n",
        "print(f\"â€¢ {high_risk_pct:.1f}% of content would be flagged as HIGH RISK\")\n",
        "print(f\"â€¢ {medium_risk_pct:.1f}% would be flagged as MEDIUM RISK\")\n",
        "print(f\"â€¢ Combined: {total_risk_pct:.1f}% would require human review\")\n",
        "\n",
        "print(f\"\\nðŸ’° Practical Implications for Advertising:\")\n",
        "print(\"âœ“ Lightweight toxicity gates could prevent problematic ad creatives from being published\")\n",
        "print(\"âœ“ Automated screening would reduce manual review workload by ~{:.1f}%\".format(100 - total_risk_pct))\n",
        "print(\"âœ“ Brand-safety incidents could be reduced by catching high-risk content early\")\n",
        "print(\"âœ“ Medium-risk content still requires human judgment for context\")\n",
        "\n",
        "print(f\"\\nâš¡ Recommended Action:\")\n",
        "print(f\"Implement a two-tier screening system:\")\n",
        "print(f\"1. AUTO-REJECT: Content with toxicity > 0.7 AND sentiment < -0.5 ({high_risk_pct:.1f}% of cases)\")\n",
        "print(f\"2. HUMAN REVIEW: Content meeting either threshold ({medium_risk_pct:.1f}% of cases)\")\n",
        "print(f\"3. AUTO-APPROVE: Low-risk content ({100 - total_risk_pct:.1f}% of cases)\")\n",
        "\n",
        "# Save results for future use\n",
        "output_filename = 'brand_safety_analysis_results.csv'\n",
        "analysis_df.to_csv(output_filename, index=False)\n",
        "print(f\"\\nðŸ’¾ Results saved to '{output_filename}'\")\n",
        "\n",
        "# Download the results to your computer\n",
        "print(f\"\\nðŸ“¥ Downloading results to your computer...\")\n",
        "files.download(output_filename)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(\"ðŸŽ‰ Your brand-safety analysis is finished!\")\n",
        "print(\"ðŸ“Š Check the visualizations above for insights\")\n",
        "print(\"ðŸ’¾ Results have been downloaded to your computer\")\n",
        "print(\"ðŸš€ You can now use these findings to build your brand-safety gate\")"
      ],
      "metadata": {
        "id": "kjnj8g5BL14z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}